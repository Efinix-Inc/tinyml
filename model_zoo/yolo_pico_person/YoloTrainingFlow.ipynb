{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tThzeowubmF2"
      },
      "source": [
        "# Dependencies\n",
        "Darknet framework requires certain dependencies needed to be installed in our Linux machine. The dependencies include: \n",
        "- Libopencv-dev (General OpenCV library)\n",
        "- Python-opencv (For image processing in python)\n",
        "- ffmpeg (For image output viewing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCXH6sZ8EndQ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#Dependencies needed for Darknet Framework \n",
        "!apt update\n",
        "!apt upgrade -y\n",
        "!uname -m && cat /etc/*release\n",
        "!gcc --version\n",
        "!uname -r\n",
        "!apt install libopencv-dev python-opencv ffmpeg\n",
        "!sudo apt-get install xxd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5CatuE2cLhN"
      },
      "source": [
        "# Pre-Training \n",
        "\n",
        "The framework that are used to perform training is called **Darknet** . It is an open source neural network framework written in C and CUDA, and able to support Yolo (You Only Look Once) model. The training flow requires a few ingredients as follows:\n",
        "\n",
        "1. Downloading training package (this consists of Darknet framework and scripts supported to generate model up to quantized tflite which will be used in our hardware)\n",
        "2. Downloading COCO dataset and labels (Refer to : https://cocodataset.org/#home for more details about COCO dataset). This will take up about ~20GB for initial download.\n",
        "3. Extracting classes from COCO dataset and seperate to only two classes (\"Person\" and \"Not Person\")\n",
        "4. Changing and appending CPU,GPU and OPENCV settings. Ensure that the google colab GPU settings is turned on (Runtime > Change Runtime Type > Choose GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fQ6abRyJFIu_"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Download the Training file that contain model and framework\n",
        "!wget -O YoloTraining.zip https://www.dropbox.com/s/u5y6jtnln16ej2s/YoloTraining.zip?dl=0\n",
        "!unzip YoloTraining.zip\n",
        "!rm -rf YoloTraining.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlweXB0FhTfV",
        "outputId": "041e6f32-a3ba-46c5-ae00-b1963bffa385"
      },
      "outputs": [],
      "source": [
        "# Download COCO dataset label\n",
        "%cd /content/YoloTraining\n",
        "!gdown 1cXZR_ckHki6nddOmcysCuuJFM--T-Q6L\n",
        "!unzip -q coco2017labels.zip\n",
        "!rm -rf coco2017labels.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMMkTnUEmcKG",
        "outputId": "04601bd1-774e-462a-c37a-04b6fef073f0"
      },
      "outputs": [],
      "source": [
        "#Download COCO dataset (Training and validation)\n",
        "%cd coco/images\n",
        "!f=\"train2017.zip\" && curl http://images.cocodataset.org/zips/$f -o $f && unzip -q $f && rm $f  # 19G, 118k images\n",
        "!f=\"val2017.zip\" && curl http://images.cocodataset.org/zips/$f -o $f && unzip -q $f && rm $f  # 1G, 5k images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bth6RrDlGifa",
        "outputId": "ac2e60fe-238d-412a-804d-59ecae1ebe64"
      },
      "outputs": [],
      "source": [
        "%cd /content/YoloTraining/\n",
        "\n",
        "#Renaming and moving dataset \n",
        "!mkdir datasets && mv ./coco/ ./datasets/\n",
        "\n",
        "#Renaming path in dataset to match the latest path after moving dataset\n",
        "!sed -i 's/.\\/images/.\\/datasets\\/coco\\/images/g' ./datasets/coco/train2017.txt\n",
        "!sed -i 's/.\\/images/.\\/datasets\\/coco\\/images/g' ./datasets/coco/val2017.txt\n",
        "!sed -i 's/.\\/images/.\\/datasets\\/coco\\/images/g' ./datasets/coco/test-dev2017.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4jfNNAXGj2s",
        "outputId": "f6af9ee4-b965-4e53-fcec-f86dacd2f7e5"
      },
      "outputs": [],
      "source": [
        "#Extract the subset of person data from coco dataset. All the models are using the same name which is \"person\", thus we can use the same file (coco.names)\n",
        "!python ./scripts/extract_dataset_subset.py --original ./datasets/coco/ --names ./models/yolo-pico_coco_person_96x96x3/coco.names --output ./datasets/coco_person/ --classes 'person'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc6D9E5SFtvv",
        "outputId": "3953db0b-787b-48fd-dbb8-bfde4ebe6191"
      },
      "outputs": [],
      "source": [
        "%cd /content/YoloTraining/darknet\n",
        "#Change lines to enable opencv and GPU. Note that you might need to change GPU settings in Makefile if you are running locally based on your GPU model.\n",
        "#Refer to Makefile comments in darknet/Makefile for more info\n",
        "!sed -i 's/OPENCV=0/OPENCV=1/g' Makefile\n",
        "!sed -i 's/GPU=0/GPU=1/g' Makefile\n",
        "!sed -i 's/CUDNN=0/CUDNN=1/g' Makefile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbVvuR4VF3js"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#Build project to append new settings\n",
        "!make -j8 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uQBOsArzARP"
      },
      "source": [
        "# Training\n",
        "\n",
        "The model is trained with 500,000 steps with 0.000010 learning rate, thus it will take around 14 hours for smaller input size, and can go up to 24 hours for bigger input size. \n",
        "\n",
        "###Training Configuration\n",
        "- The training is set to use one GPU, as colab machine only supports one GPU per machine.\n",
        "- The training parameters and layer adjustment can be made by changing the parameters in .cfg file. E.g;\n",
        "  -  ./models/yolo-pico_coco_person_96x96x3/yolo-pico_coco_person_96x96x3.cfg: \n",
        "\n",
        "```\n",
        "[net]\n",
        "batch=32\n",
        "subdivisions=1\n",
        "width=96\n",
        "height=96\n",
        "channels=3\n",
        "momentum=0.949\n",
        "decay=0.0005\n",
        "angle=0\n",
        "saturation=1.5\n",
        "exposure=1.5\n",
        "hue=.1\n",
        "```\n",
        "- These are the training parameters which will be consumed during training runtime.\n",
        "\n",
        "\n",
        "###Method of trainings available:\n",
        "1.Training from scratch \n",
        "- This will start the training from step 1, and will go up to 500K steps by default (following what we set in .cfg file).\n",
        "2.Using pre-trained weights \n",
        "- this will load weight which has been trained previously, and continue to train from that point. In this case, our last trained model is trained up to 500K steps.\n",
        "\n",
        "\n",
        "\n",
        "These are the sample of completing the training snapshot: \n",
        "The sample of last 3 steps before completing the training is shown below:\n",
        "\n",
        "\n",
        "```\n",
        " 499998: 1.113633, 1.115932 avg loss, 0.000010 rate, 0.069034 seconds, 15999936 images, 0.002009 hours left\n",
        "Loaded: 0.000079 seconds\n",
        "v3 (iou loss, Normalizer: (iou: 0.07, obj: 1.00, cls: 1.00) Region 59 Avg (IOU: 0.749662), count: 19, class_loss = 0.275771, iou_loss = 0.027597, total_loss = 0.303368 \n",
        "v3 (iou loss, Normalizer: (iou: 0.07, obj: 1.00, cls: 1.00) Region 66 Avg (IOU: 0.417471), count: 171, class_loss = 2.373719, iou_loss = 1.208714, total_loss = 3.582433 \n",
        " total_bbox = 84496405, rewritten_bbox = 17.515007 % \n",
        "\n",
        " 499999: 1.325656, 1.136904 avg loss, 0.000010 rate, 0.070176 seconds, 15999968 images, 0.001989 hours left\n",
        "Loaded: 0.000121 seconds\n",
        "v3 (iou loss, Normalizer: (iou: 0.07, obj: 1.00, cls: 1.00) Region 59 Avg (IOU: 0.739210), count: 19, class_loss = 0.256957, iou_loss = 0.029729, total_loss = 0.286686 \n",
        "v3 (iou loss, Normalizer: (iou: 0.07, obj: 1.00, cls: 1.00) Region 66 Avg (IOU: 0.485464), count: 120, class_loss = 2.001330, iou_loss = 0.876470, total_loss = 2.877800 \n",
        " total_bbox = 84496544, rewritten_bbox = 17.514999 % \n",
        "\n",
        " 500000: 1.129992, 1.136213 avg loss, 0.000010 rate, 0.064493 seconds, 16000000 images, 0.001969 hours left\n",
        "Saving weights to models/yolo-pico_coco_person_96x96x3_500000.weights\n",
        "Saving weights to models/yolo-pico_coco_person_96x96x3_last.weights\n",
        "Saving weights to models/yolo-pico_coco_person_96x96x3_final.weights\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YR0iaPfgH3E_"
      },
      "outputs": [],
      "source": [
        "%cd /content/YoloTraining\n",
        "#Perform training on model. Specify higher number of GPUs if available ; e.g -gpus 0,1,2,3\n",
        "#Note that if training from scratch, it is adviced to disabled the output by using %%capture as it might overflow and hang\n",
        "\n",
        "##1.Training from scratch\n",
        "#%%capture\n",
        "# !./darknet/darknet detector train ./models/yolo-pico_coco_person_96x96x3/coco.data ./models/yolo-pico_coco_person_96x96x3/yolo-pico_coco_person_96x96x3.cfg  -gpus 0 -dont_show\n",
        "\n",
        "##2.Pre-trained weights\n",
        "!./darknet/darknet detector train ./models/yolo-pico_coco_person_96x96x3/coco.data ./models/yolo-pico_coco_person_96x96x3/yolo-pico_coco_person_96x96x3.cfg ./models/yolo-pico_coco_person_96x96x3/yolo-pico_coco_person_96x96x3.weights -gpus 0 -dont_show \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2-QwEQwduUN"
      },
      "source": [
        "# Post-Training \n",
        "\n",
        "Upon training the model, we will get a final weighted model , which will be converted to h5 format , followed by tflite model. Before converting , we can do a few evaluation/verification steps to ensure our model is accurate enough to deploy. The post-training steps are as follows:\n",
        "\n",
        "### Evaluation\n",
        "- Evaluate the accuracy of model using validation dataset (we will get the precision, recall, f1-score, IoU and mAP score)\n",
        "- Evaluation at single test image (This will allow us to see the score between person and non-person and can visualize the bounding box drawn on the image using openCV)\n",
        "\n",
        "\n",
        "\n",
        "### Model conversion\n",
        "\n",
        "- The model is converted as following the flow:\n",
        "\n",
        "(final weight) > (Keras .h5 model) > (TFLite .tflite model) \n",
        "\n",
        "- The .tflite model is quantized using post-training quantization, which the model will be used to deploy in hardware.\n",
        "- Upon converting to quantized TFLite model, we will generate the model in .cc file (in C format) before deploying to the hardware.\n",
        "- At the end of the steps, we will get two files , the .cc model which contain the model information (mostly in hexadecimal), and .h which contains the variable used by the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcUYVZFTLF8C"
      },
      "outputs": [],
      "source": [
        "#Accuracy Evaluation of Trained Models\n",
        "!./darknet/darknet detector map ./models/yolo-pico_coco_person_96x96x3/coco.data ./models/yolo-pico_coco_person_96x96x3/yolo-pico_coco_person_96x96x3.cfg ./models/yolo-pico_coco_person_96x96x3_final.weights -points 101"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9SyuF-hLgWY"
      },
      "outputs": [],
      "source": [
        "#Evaluation of Trained Models Using Test Images\n",
        "!./darknet/darknet detector test ./models/yolo-pico_coco_person_96x96x3/coco.data ./models/yolo-pico_coco_person_96x96x3/yolo-pico_coco_person_96x96x3.cfg ./models/yolo-pico_coco_person_96x96x3_final.weights ./darknet/data/person.jpg -i 0 -thresh 0.25 -dont_show "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWdpaP1EMaXI"
      },
      "outputs": [],
      "source": [
        "#View output image after prediction. Bounding box will be shown that shows person is detected\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "img = cv2.imread('predictions.jpg', cv2.IMREAD_UNCHANGED)\n",
        "cv2_imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cf7r-s87Mn8I"
      },
      "outputs": [],
      "source": [
        "#Model Conversion to Keras H5 Format\n",
        "!python ./scripts/convert.py ./models/yolo-pico_coco_person_96x96x3/yolo-pico_coco_person_96x96x3.cfg ./models/yolo-pico_coco_person_96x96x3_final.weights yolo-pico_coco_person_96x96x3.h5 -f\n",
        "\n",
        "#Move the h5 file to model folder\n",
        "!mv ./yolo-pico_coco_person_96x96x3.h5 ./models/yolo-pico_coco_person_96x96x3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQseS4WnOjBK"
      },
      "outputs": [],
      "source": [
        "#Model Conversion to Quantized Int8 TFLite Format\n",
        "!python ./scripts/convert_darknet_tflite.py --model yolo-pico_coco_person_96x96x3 --images ./datasets/coco/val2017.txt --input_shape 96x96x3 --sample ./datasets/coco/images/val2017/000000018380.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1aYdUOQhtbC"
      },
      "outputs": [],
      "source": [
        "#Generate .cc and .h file to be used for hardware implementation\n",
        "%cd /content\n",
        "!xxd -i /content/YoloTraining/models/yolo-pico_coco_person_96x96x3/96x96x3/yolo-pico_coco_person_96x96x3_quant.tflite > network_model_data_tmp.cc\n",
        "!wget -O convert_model_cc.py https://www.dropbox.com/s/1q8n4jm9fk4gzzf/convert_model_cc.py?dl=0\n",
        "!python convert_model_cc.py --network=\"yolo_pico\" --application=\"person_detect\"\n",
        "!rm -rf network_model_data_tmp.cc"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "tThzeowubmF2"
      ],
      "machine_shape": "hm",
      "name": "YoloTrainingCode.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
